(self.webpackChunk=self.webpackChunk||[]).push([[40850],{863020:function(i){"use strict";i.exports=JSON.parse('{"bagging-description":"L\'aggregazione di Bootstrap \xe8 spesso utilizzata con metodi ad albero decisionale. Disegnando campioni di bootstrap dai dati di formazione originali che stimano un modello per ogni campione disegnato, le previsioni di tali modelli sono poi mediate per produrre previsioni a bassa variazione non cos\xec inclini all\'overfitting.","boosting-description":"Gli algoritmi di potenziamento, come AdaBoost, imparano iterativamente i classificatori deboli e li aggiungono a un classificatore finale forte ponderandoli. Aggiungendo i classificatori, i dati di input classificati male ottengono un peso pi\xf9 alto e gli esempi che sono classificati correttamente perdono peso. Cos\xec, i futuri studenti deboli si concentrano maggiormente sugli esempi che i precedenti studenti deboli hanno erroneamente classificato.","cart-description":"L\'analisi dell\'albero di classificazione e regressione (CART) costruisce un albero che va dalle caratteristiche di un\'osservazione (rappresentata nei rami) ad un valore previsto (rappresentato nelle foglie).","classification":"Classificazione","clustering":"Clustering","dimensionality-reduction":"Riduzione della dimensionalit\xe0","elastic-net-description":"La rete elastica \xe8 un metodo di regressione regolarizzato che combina linearmente le penalit\xe0 L1 e L2 del lazo e della cresta.","ensemble":"Ensemble","kmeans-description":"Raggruppare le osservazioni in un numero fisso (k) di cluster in modo tale che i membri del cluster siano pi\xf9 simili tra loro che non alle osservazioni in altri cluster.","knn-description":"Utilizzato sia per la regressione che per la classificazione. Utilizza il voto di maggioranza tra i punti k-nearest per la classificazione kNN. Per la regressione, l\'output \xe8 la media dei valori k-nearest.","lasso-description":"Metodo di regressione regolarizzato che penalizza i coefficienti di regressione utilizzando la norma L1. Scambia la varianza inferiore per un po\' di distorsione. Porta ad un modello rado.","linear-regression-description":"Modella la relazione tra una risposta scalare e una o pi\xf9 variabili esplicative. La regressione lineare semplice si riferisce al caso in cui \xe8 presente un solo predittore, la regressione lineare multipla viene utilizzata con pi\xf9 variabili esplicative. Si chiama lineare perch\xe9 la funzione stimata \xe8 lineare nei suoi coefficienti.","logistic-regression-description":"La regressione logistica \xe8 un metodo di classificazione utilizzato per assegnare le osservazioni a una delle due classi. La regressione logistica utilizza la funzione sigmoide logistica per restituire un valore di probabilit\xe0 per ogni classe","naive-bayes-description":"I metodi Naive Bayes sono un insieme di algoritmi di classificazione basati sull\'applicazione del teorema di Bayes con l\'assunzione \\"ingenua\\" dell\'indipendenza condizionale tra ogni coppia di caratteristiche dato il valore della variabile di classe.","neural-networks-description":"Le reti neurali artificiali sono utilizzate per modellare complesse relazioni tra gli ingressi e le uscite attraverso l\'apprendimento di una funzione non lineare senza l\'ingegneria manuale delle caratteristiche.","random-forest-description":"Le Foreste Casuali costruiscono una moltitudine di alberi decisionali al momento dell\'addestramento e restituiscono la classe che \xe8 la modalit\xe0 delle classi (classificazione) o la previsione media (regressione) dei singoli alberi.","regression":"Regressione","ridge-description":"Metodo di regressione regolarizzato che penalizza i coefficienti di regressione utilizzando la norma L2. Scambia la varianza inferiore per un po\' di distorsione. Non produce un modello rado, cio\xe8 i coefficienti non vengono portati a zero.","svm-description":"Le Macchine Vettoriali di Supporto sono classificatori discriminanti. Dati i dati di formazione etichettati, l\'algoritmo trova un iperpiano ottimale per classificare i nuovi esempi. In due dimensioni questo iperpiano \xe8 una linea che lo divide in due parti.","\xfcca-description":"L\'Analisi delle Componenti Principali (PCA) utilizza una trasformazione ortogonale per convertire le variabili eventualmente correlate in un insieme di valori di variabili linearmente non correlate, chiamate componenti principali."}')}}]);