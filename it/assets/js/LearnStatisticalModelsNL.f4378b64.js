(window.webpackJsonp=window.webpackJsonp||[]).push([[627],{3329:function(e){e.exports=JSON.parse('{"bagging-description":"Bootstrap aggregatie wordt vaak gebruikt met beslissingsboom methoden. Door het trekken van bootstrap-monsters uit de oorspronkelijke trainingsgegevens, waarbij voor elk getrokken monster een model wordt geschat, worden de voorspellingen van die modellen vervolgens gemiddeld genomen om lagere variantie-voorspellingen te geven die niet zo gevoelig zijn voor overfitting.","boosting-description":"Het stimuleren van algoritmen zoals AdaBoost leert iteratief zwakke classifiers en voegt ze toe aan een laatste sterke classifier door ze te wegen. Naarmate classifiers worden toegevoegd, krijgen verkeerd ingevoerde gegevens een hoger gewicht en verliezen voorbeelden die correct zijn geclassificeerd gewicht. Toekomstige zwakke leerlingen richten zich dus meer op de voorbeelden die eerdere zwakke leerlingen verkeerd geclassificeerd hebben.","cart-description":"Classificatie- en Regressieboom (CART) analyse bouwt een boom die gaat van de kenmerken van een observatie (vertegenwoordigd in de takken) naar een voorspelde waarde (vertegenwoordigd in de bladeren).","classification":"Classificatie","clustering":"Clustering","dimensionality-reduction":"Dimensievermindering","elastic-net-description":"Elastisch net is een geregulariseerde regressiemethode die de L1- en L2-straf van de lasso- en ridge-methoden lineair combineert.","ensemble":"Ensemble","kmeans-description":"Groepswaarnemingen in een vast aantal (k) clusters, zodat de clusterleden meer op elkaar lijken dan op waarnemingen in andere clusters.","knn-description":"Gebruikt voor zowel regressie als classificatie. Gebruikt meerderheidsstem onder de k-nearest punten voor kNN classificatie. Voor regressie is de output het gemiddelde van de k-nearest waarden.","lasso-description":"Geregulariseerde regressiemethode die regressieco\xebffici\xebnten bestraft met behulp van de L1-norm. Handelt lagere variantie voor een beetje vertekening. Leidt tot een spaarzaam model.","linear-regression-description":"Modelleert de relatie tussen een scalaire respons en een of meer verklarende variabelen. Eenvoudige lineaire regressie verwijst naar het geval waarin \xe9\xe9n voorspeller aanwezig is, meerdere lineaire regressies worden gebruikt met meerdere verklarende variabelen. Het wordt lineair genoemd omdat de geschatte functie lineair is in zijn co\xebffici\xebnten.","logistic-regression-description":"Logistieke regressie is een classificatiemethode die wordt gebruikt om waarnemingen toe te wijzen aan een van de twee klassen. Logistieke regressie maakt gebruik van de logistieke sigmoid-functie om een waarschijnlijkheidswaarde voor elke klasse terug te geven.","naive-bayes-description":"Na\xefeve Bayes-methoden zijn een reeks classificatiealgoritmen die gebaseerd zijn op het toepassen van de stelling van Bayes met de \\"na\xefeve\\" aanname van voorwaardelijke onafhankelijkheid tussen elk paar kenmerken gezien de waarde van de klassenvariabele.","neural-networks-description":"Kunstmatige neurale netwerken worden gebruikt om complexe relaties tussen inputs en outputs te modelleren door een niet-lineaire functie te leren zonder manuele feature engineering.","random-forest-description":"Random Forests construeren een veelheid aan beslissingsbomen op het moment van de training en geven de klasse terug die de modus van de klassen (classificatie) of de gemiddelde voorspelling (regressie) van de individuele bomen is.","regression":"Regressie","ridge-description":"Geregulariseerde regressiemethode die regressieco\xebffici\xebnten met behulp van de L2-norm bestraft. Handelt lagere variantie voor een beetje vertekening. Geeft geen spaarzaam model, d.w.z. dat co\xebffici\xebnten niet tot nul worden gedreven.","svm-description":"Support Vector Machines zijn discriminerende classificaties. Gezien de gelabelde trainingsgegevens vindt het algoritme een optimaal hypervlak om nieuwe voorbeelden te categoriseren. In twee dimensies is dit hypervlak een lijn die het in twee\xebn deelt.","\xfcca-description":"Principal Component Analysis (PCA) maakt gebruik van een orthogonale transformatie om eventueel gecorreleerde variabelen om te zetten in een reeks waarden van lineair niet gecorreleerde variabelen, de zogenaamde hoofdcomponenten."}')}}]);