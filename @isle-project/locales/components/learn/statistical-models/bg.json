{
	"bagging-description": "Агрегирането на Bootstrap често се използва при методите на дърветата на решенията. При изготвяне на бутстрап извадки от оригиналните данни за обучение, като за всяка извадка се оценява модел, прогнозите на посочените модели се осредняват, за да се получат прогнози с по-ниска вариация, които не са толкова склонни към свръхподготовка.",
	"boosting-description": "Алгоритмите за усилване, като например AdaBoost, итеративно изучават слаби класификатори и ги добавят към крайния силен класификатор, като им придават тежест. С добавянето на класификатори неправилно класифицираните входни данни получават по-голяма тежест, а примерите, които са класифицирани правилно, губят тежест. По този начин бъдещите слаби класификатори се фокусират повече върху примерите, които предишните слаби класификатори са класифицирали погрешно.",
	"cart-description": "Анализът на класификационни и регресионни дървета (CART) изгражда дърво, което преминава от характеристиките на дадено наблюдение (представени в клоните) до прогнозираната стойност (представена в листата).",
	"classification": "Класификация",
	"clustering": "Създаване на клъстери",
	"dimensionality-reduction": "Намаляване на размерността",
	"elastic-net-description": "Еластичната мрежа е регулиран регресионен метод, който линейно комбинира наказанията L1 и L2 на методите lasso и ridge.",
	"ensemble": "Ансамбъл",
	"kmeans-description": "Групиране на наблюденията във фиксиран брой (k) клъстери, така че членовете на клъстера да са по-сходни помежду си, отколкото с наблюденията в други клъстери.",
	"knn-description": "Използва се както за регресия, така и за класификация. Използва гласуване с мнозинство сред k-близки точки за kNN класификация. За регресия изходът е средната стойност на k-най-близките стойности.",
	"lasso-description": "Метод за регуларизирана регресия, който наказва регресионните коефициенти, като използва нормата L1. Заменя се по-ниска дисперсия за малко отклонение. Води до рядък модел.",
	"linear-regression-description": "Моделира връзката между скаларен отговор и една или повече обяснителни променливи. Обикновената линейна регресия се отнася за случая, в който е налице един предиктор, а множествената линейна регресия се използва при множество обяснителни променливи. Нарича се линейна, защото оценената функция е линейна по отношение на своите коефициенти.",
	"logistic-regression-description": "Логистичната регресия е метод за класификация, който се използва за отнасяне на наблюденията към един от двата класа. Логистичната регресия използва логистичната сигмоидна функция, за да върне стойност на вероятността за всеки клас",
	"naive-bayes-description": "Методите на наивния Бейс са набор от алгоритми за класификация, основани на прилагането на теоремата на Бейс с \"наивното\" допускане за условна независимост между всяка двойка признаци, като се има предвид стойността на променливата на класа.",
	"neural-networks-description": "Изкуствените невронни мрежи се използват за моделиране на сложни връзки между входовете и изходите чрез изучаване на нелинейна функция без ръчно проектиране на функции.",
	"pca-description": "Анализът на главните компоненти (PCA) използва ортогонална трансформация, за да превърне евентуално корелирани променливи в набор от стойности на линейно некорелирани променливи, наречени главни компоненти.",
	"random-forest-description": "Случайните гори конструират множество дървета на решения по време на обучението и връщат класа, който е моделът на класовете (класификация) или средната прогноза (регресия) на отделните дървета.",
	"regression": "Регресия",
	"ridge-description": "Метод за регуларизирана регресия, който наказва регресионните коефициенти, като използва нормата L2. Заменя се по-ниска дисперсия за малко отклонение. Не води до разреждане на модела, т.е. коефициентите не се свеждат до нула.",
	"svm-description": "Машините с поддържащи вектори са дискриминативни класификатори. При наличие на маркирани данни за обучение алгоритъмът намира оптимална хиперплоскост за категоризиране на новите примери. В две измерения тази хиперплоскост е линия, разделяща я на две части.",
	"üca-description": "Анализът на главните компоненти (PCA) използва ортогонална трансформация, за да превърне евентуално корелирани променливи в набор от стойности на линейно некорелирани променливи, наречени главни компоненти."
}
