{
	"bagging-description": "ブートストラップ集計は，決定木法でよく使用される．元の学習データからブートストラップサンプルを引き、引き出されたサンプルごとにモデルを推定し、前記モデルの予測値を平均化して、オーバーフィットしない低分散の予測値を得ます。",
	"boosting-description": "AdaBoostのようなブーストアルゴリズムは、弱い分類器を反復的に学習し、それらに重みをつけることで最終的な強い分類器に追加します。分類器が追加されると、誤分類された入力データはより高い重みを増し、正しく分類された例は重みを失う。このようにして、将来の弱い学習者は、以前の弱い学習者が誤分類した例により多くの例に焦点を当てることになります。",
	"cart-description": "分類と回帰の木 (CART) 解析は、観測の特徴 (枝で表される) から予測値 (葉で表される) までの木を構築します。",
	"classification": "分類",
	"clustering": "クラスタリング",
	"dimensionality-reduction": "次元削減",
	"elastic-net-description": "弾性ネットは、ラッソ法とリッジ法のL1とL2のペナルティを線形に組み合わせた正則化回帰法です。",
	"ensemble": "アンサンブル",
	"kmeans-description": "クラスタ・メンバーが他のクラスタのオブザベーションよりも互いに類似しているようなクラスタの固定数(k)にオブザベーションをグループ化する．",
	"knn-description": "回帰と分類の両方に使用されます。kNN 分類には，k-nearest 点間の多数決を用いる．回帰では，k-nearest値の平均が出力されます．",
	"lasso-description": "L1ノルムを使用して回帰係数にペナルティを与える正則化回帰法．少しのバイアスのために，より低い分散を交換する．疎なモデルになる．",
	"linear-regression-description": "スカラー応答と1つ以上の説明変数との関係をモデル化する。単純線形回帰は予測変数が1つの場合を指し、重回帰は複数の説明変数を用いて行う。推定された関数の係数が線形であることから線形と呼ばれる。",
	"logistic-regression-description": "ロジスティック回帰は，オブザベーションを2つのクラスのいずれかに割り当てるために使用される分類法である．ロジスティック回帰は、各クラスの確率値を返すために、ロジスティック・シグモイド関数を使用する",
	"naive-bayes-description": "ナイーブベイズ法は、クラス変数の値を与えられた特徴のすべてのペア間の条件付き独立性という「ナイーブ」な仮定でベイズの定理を適用することに基づく分類アルゴリズムのセットです。",
	"neural-networks-description": "人工ニューラルネットワークは、手動の特徴工学を用いずに非線形関数を学習することで、入力と出力の間の複雑な関係をモデル化するために使用されます。",
	"random-forest-description": "ランダムフォレストは、学習時に多数の決定木を構築し、個々の木のクラス（分類）または平均予測（回帰）のモードであるクラスを返す。",
	"regression": "回帰",
	"ridge-description": "L2ノルムを使用して回帰係数にペナルティを与える正則化回帰法．少しのバイアスのために，より低い分散を交換する．疎なモデルは得られない，すなわち，係数がゼロに駆動されない．",
	"svm-description": "サポートベクターマシンは識別分類器です。ラベル付けされた学習データが与えられると、アルゴリズムは、新しい例を分類するための最適な超平面を見つけます。2次元では、この超平面は2つの部分に分割された線である。",
	"üca-description": "主成分分析（PCA）は、直交変換を使用して、相関のある可能性のある変数を、主成分と呼ばれる線形の相関のない変数の値の集合に変換します。"
}
