{
	"bagging-description": "L'agrégation bootstrap est souvent utilisée avec les méthodes d'arbre de décision. En tirant des échantillons bootstrap des données de formation originales, en estimant un modèle pour chaque échantillon tiré, les prédictions desdits modèles sont ensuite moyennées pour donner des prédictions à faible variance moins sujettes au surajustement.",
	"boosting-description": "Les algorithmes de boosting tels qu'AdaBoost apprennent itérativement les classificateurs faibles et les ajoutent à un classificateur fort final en les pondérant. À mesure que des classificateurs sont ajoutés, les données d'entrée mal classées prennent plus de poids et les exemples qui sont classés correctement perdent du poids. Ainsi, les futurs apprenants faibles se concentrent davantage sur les exemples que les apprenants faibles précédents ont mal classés.",
	"cart-description": "L'analyse CART (Classification And Regression Tree) construit un arbre qui va des caractéristiques d'une observation (représentées dans les branches) à une valeur prédite (représentée dans les feuilles).",
	"classification": "Classification",
	"clustering": "Regroupement",
	"dimensionality-reduction": "Réduction de la dimensionnalité",
	"elastic-net-description": "Le filet élastique est une méthode de régression régularisée qui combine linéairement les pénalités L1 et L2 des méthodes du lasso et des crêtes.",
	"ensemble": "Ensemble",
	"kmeans-description": "Regrouper les observations en un nombre fixe (k) de groupes de manière à ce que les membres des groupes soient plus semblables les uns aux autres qu'aux observations dans d'autres groupes.",
	"knn-description": "Utilisé à la fois pour la régression et la classification. Utilise le vote majoritaire parmi les points k les plus proches pour la classification kNN. Pour la régression, le résultat est la moyenne des valeurs k les plus proches.",
	"lasso-description": "Méthode de régression régularisée qui pénalise les coefficients de régression en utilisant la norme L1. Traite une variance plus faible pour un peu de biais. Conduit à un modèle épars.",
	"linear-regression-description": "Modélisation de la relation entre une réponse scalaire et une ou plusieurs variables explicatives. La régression linéaire simple fait référence au cas où un prédicteur est présent, la régression linéaire multiple est utilisée avec de multiples variables explicatives. Elle est appelée linéaire parce que la fonction estimée est linéaire dans ses coefficients.",
	"logistic-regression-description": "La régression logistique est une méthode de classification utilisée pour attribuer des observations à l'une ou l'autre des deux classes. La régression logistique utilise la fonction sigmoïde logistique pour renvoyer une valeur de probabilité pour chaque classe",
	"naive-bayes-description": "Les méthodes naïves de Bayes sont un ensemble d'algorithmes de classification basés sur l'application du théorème de Bayes avec l'hypothèse \"naïve\" d'indépendance conditionnelle entre chaque paire de caractéristiques étant donné la valeur de la variable de classe.",
	"neural-networks-description": "Les réseaux neuronaux artificiels sont utilisés pour modéliser des relations complexes entre les entrées et les sorties en apprenant une fonction non linéaire sans ingénierie manuelle des caractéristiques.",
	"pca-description": "L'analyse en composantes principales (ACP) utilise une transformation orthogonale pour convertir des variables éventuellement corrélées en un ensemble de valeurs de variables linéairement non corrélées appelées composantes principales.",
	"random-forest-description": "Random Forests construit une multitude d'arbres de décision au moment de l'entraînement et retourne la classe qui est le mode des classes (classification) ou la prédiction moyenne (régression) des arbres individuels.",
	"regression": "Régression",
	"ridge-description": "Méthode de régression régularisée qui pénalise les coefficients de régression en utilisant la norme L2. Elle négocie une variance plus faible pour un peu de biais. Ne produit pas un modèle peu dense, c'est-à-dire que les coefficients ne sont pas ramenés à zéro.",
	"svm-description": "Les machines à vecteurs de soutien sont des classificateurs discriminants. À partir de données d'entraînement étiquetées, l'algorithme trouve un hyperplan optimal pour catégoriser de nouveaux exemples. En deux dimensions, cet hyperplan est une ligne qui le divise en deux parties.",
	"üca-description": "L'analyse en composantes principales (ACP) utilise une transformation orthogonale pour convertir des variables éventuellement corrélées en un ensemble de valeurs de variables linéairement non corrélées appelées composantes principales."
}
